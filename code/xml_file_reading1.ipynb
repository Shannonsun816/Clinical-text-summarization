{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f267aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from docx import Document\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from lxml import etree\n",
    "from docx.shared import Pt\n",
    "from docx.enum.style import WD_STYLE_TYPE\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.text import WD_BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68989160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(xml_file):\n",
    "    tree = etree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    data = {\n",
    "        'brief_summary': 'NA',\n",
    "        'detailed_description': 'NA',\n",
    "        'brief_title': 'NA',\n",
    "        'conditions': [],\n",
    "        'eligibility_criteria': 'NA',\n",
    "        'eligibility_gender': 'NA',\n",
    "        'eligibility_maximum_age': 'NA',\n",
    "        'eligibility_minimum_age': 'NA',\n",
    "        'id_info_nct_id': 'NA',\n",
    "        'id_info_org_study_id': 'NA',\n",
    "        'location_countries': [],\n",
    "        'location_contact_email': 'NA',\n",
    "        'location_contact_last_name': 'NA',\n",
    "        'primary_outcome_description': 'NA',\n",
    "        'primary_outcome_measure': 'NA',\n",
    "        'primary_outcome_time_frame': 'NA',\n",
    "        'provided_document_url': 'NA',\n",
    "        'secondary_outcome_description': 'NA',\n",
    "        'secondary_outcome_measure': 'NA',\n",
    "        'secondary_outcome_time_frame': 'NA',\n",
    "    }\n",
    "\n",
    "    for elem in root.findall('./brief_summary/textblock'):\n",
    "        data['brief_summary'] = elem.text.strip()\n",
    "    \n",
    "    for elem in root.findall('./detailed_description/textblock'):\n",
    "        detailed_description = elem.text.strip()\n",
    "        summary = extractive_summarization(detailed_description)\n",
    "        data['detailed_description'] = summary\n",
    "\n",
    "    for elem in root.findall('./brief_title'):\n",
    "        data['brief_title'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./condition_browse/mesh_term'):\n",
    "        data['conditions'].append(elem.text.strip())\n",
    "\n",
    "    for elem in root.findall('./eligibility/criteria/textblock'):\n",
    "        eligibility_criteria = elem.text.strip()\n",
    "        summary1 = extractive_summarization(eligibility_criteria)\n",
    "        words = summary1.split()\n",
    "        summary2 = \"\".join(summary1)\n",
    "        data['eligibility_criteria'] = summary2\n",
    "\n",
    "    for elem in root.findall('./eligibility/gender'):\n",
    "        data['eligibility_gender'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./eligibility/maximum_age'):\n",
    "        data['eligibility_maximum_age'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./eligibility/minimum_age'):\n",
    "        data['eligibility_minimum_age'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./id_info/nct_id'):\n",
    "        data['id_info_nct_id'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./id_info/org_study_id'):\n",
    "        data['id_info_org_study_id'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./location_countries/country'):\n",
    "        data['location_countries'].append(elem.text.strip())\n",
    "\n",
    "    for elem in root.findall('./location/contact/email'):\n",
    "        data['location_contact_email'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./location/contact/last_name'):\n",
    "        data['location_contact_last_name'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./primary_outcome/description'):\n",
    "        data['primary_outcome_description'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./primary_outcome/measure'):\n",
    "        data['primary_outcome_measure'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./primary_outcome/time_frame'):\n",
    "        data['primary_outcome_time_frame'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./provided_document_section/provided_document/document_url'):\n",
    "        data['provided_document_url'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./secondary_outcome/description'):\n",
    "        data['secondary_outcome_description'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./secondary_outcome/measure'):\n",
    "        data['secondary_outcome_measure'] = elem.text.strip()\n",
    "\n",
    "    for elem in root.findall('./secondary_outcome/time_frame'):\n",
    "        data['secondary_outcome_time_frame'] = elem.text.strip()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2d6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",

    "\"\"\"\n",
    "def extractive_summarization(text, num_sentences=1):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "    \n",
    "    freq = FreqDist(words)\n",
    "    ranking = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        ranking[i] = 0\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in freq:\n",
    "                ranking[i] += freq[word]\n",
    "\n",
    "    top_sentences = nlargest(num_sentences, ranking, key=ranking.get)\n",
    "    summary = ' '.join(sentences[i] for i in sorted(top_sentences))\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8880f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extracted_data(data):\n",
    "    print(\"Brief Summary:\", data['brief_summary'])\n",
    "    print(\"Detailed Description:\", data['detailed_description'])\n",
    "    print(\"Brief Title:\", data['brief_title'])\n",
    "    print(\"Conditions:\", ', '.join(data['conditions']))\n",
    "    print(\"Eligibility Criteria:\", data['eligibility_criteria'])\n",
    "    print(\"Eligibility Gender:\", data['eligibility_gender'])\n",
    "    print(\"Eligibility Maximum Age:\", data['eligibility_maximum_age'])\n",
    "    print(\"Eligibility Minimum Age:\", data['eligibility_minimum_age'])\n",
    "    print(\"ID Info NCT ID:\", data['id_info_nct_id'])\n",
    "    print(\"ID Info Org Study ID:\", data['id_info_org_study_id'])\n",
    "    print(\"Location Countries:\", ', '.join(data['location_countries']))\n",
    "    print(\"Location Contact Email:\", data['location_contact_email'])\n",
    "    print(\"Location Contact Last Name:\", data['location_contact_last_name'])\n",
    "    print(\"Primary Outcome Description:\", data['primary_outcome_description'])\n",
    "    print(\"Primary Outcome Measure:\", data['primary_outcome_measure'])\n",
    "    print(\"Primary Outcome Time Frame:\", data['primary_outcome_time_frame'])\n",
    "    print(\"Provided Document URL:\", data['provided_document_url'])\n",
    "    print(\"Secondary Outcome Description:\", data['secondary_outcome_description'])\n",
    "    print(\"Secondary Outcome Measure:\", data['secondary_outcome_measure'])\n",
    "    print(\"Secondary Outcome Time Frame:\", data['secondary_outcome_time_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa8b0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_word(doc, data, file_name):\n",
    "    doc.add_heading(file_name, level=1)\n",
    "    for key, value in data.items():\n",
    "        doc.add_heading(key.replace('_', ' ').title(), level=2)\n",
    "        doc.add_paragraph(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "387a7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style(document, style_name, font_name, font_size):\n",
    "    style = document.styles[style_name]\n",
    "    font = style.font\n",
    "    font.name = font_name\n",
    "    font.size = Pt(font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61848cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lines_and_newlines(doc):\n",
    "    new_doc = Document()\n",
    "    set_style(new_doc, 'Normal', 'Calibri', 12)\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        if text != '':\n",
    "            # remove more space\n",
    "            cleaned_text = ' '.join(text.split())\n",
    "            new_p = new_doc.add_paragraph(cleaned_text)\n",
    "            new_p.style = paragraph.style\n",
    "\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dfdd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data for folder NCT0000xxxx to NCT0000xxxx.docx\n",
      "Saved data for folder NCT0001xxxx to NCT0001xxxx.docx\n"
     ]
    }
   ],
   "source": [
    "for yyyy in range(1000):\n",
    "    folder_name = f'NCT{yyyy:04d}xxxx'\n",
    "    if not os.path.exists(folder_name):\n",
    "        continue\n",
    "        \n",
    "    doc = Document()\n",
    "    set_style(doc, 'Normal', 'Calibri', 12)\n",
    "    for xxxx in range(10000):\n",
    "        file_name = f'NCT{yyyy:04d}{xxxx:04d}.xml'\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "            \n",
    "        data = extract_data(file_path)\n",
    "        save_data_to_word(doc, data, file_name)\n",
    "    \n",
    "    doc = remove_empty_lines_and_newlines(doc)\n",
    "    output_file_name = f'{folder_name}.docx'\n",
    "    doc.save(output_file_name)\n",
    "    print(f'Saved data for folder {folder_name} to {output_file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
